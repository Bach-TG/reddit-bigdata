# ðŸ—ï¸ Reddit Streaming Pipeline - Complete Architecture

## ðŸ“Š System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         REDDIT DATA SOURCE                               â”‚
â”‚                    (API or JSONL Batch Files)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      AIRFLOW ORCHESTRATION                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Health   â”‚  â”‚   Fetch    â”‚  â”‚  Produce   â”‚  â”‚  Monitor   â”‚       â”‚
â”‚  â”‚   Checks   â”‚â†’ â”‚    Data    â”‚â†’ â”‚  to Kafka  â”‚â†’ â”‚   Spark    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                          â”‚
â”‚  DAG: reddit_streaming_pipeline                                        â”‚
â”‚  Schedule: Every 4 hours                                                â”‚
â”‚  Components: 9 tasks with dependencies                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         KAFKA MESSAGE QUEUE                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Topic: reddit-posts                                          â”‚      â”‚
â”‚  â”‚  Partitions: 3 (configurable)                                â”‚      â”‚
â”‚  â”‚  Replication: 1 (increase in production)                     â”‚      â”‚
â”‚  â”‚  Retention: 24 hours                                          â”‚      â”‚
â”‚  â”‚  Compression: gzip                                            â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                          â”‚
â”‚  Producer Config:                                                       â”‚
â”‚  - Batch size: 16KB                                                     â”‚
â”‚  - Acks: all (wait for all replicas)                                   â”‚
â”‚  - Retries: 3                                                           â”‚
â”‚  - Compression: gzip                                                    â”‚
â”‚                                                                          â”‚
â”‚  Monitoring: Kafka UI (http://localhost:8090)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SPARK STRUCTURED STREAMING                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Master: 1 node                                               â”‚      â”‚
â”‚  â”‚  Workers: 2 nodes Ã— (2 cores, 2GB RAM each)                 â”‚      â”‚
â”‚  â”‚  Total: 4 cores, 4GB RAM                                     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                          â”‚
â”‚  Streaming Config:                                                      â”‚
â”‚  - Trigger: 10 seconds (micro-batch)                                   â”‚
â”‚  - Checkpoint: /tmp/spark-checkpoint                                   â”‚
â”‚  - Output mode: append                                                  â”‚
â”‚  - Kafka offset: earliest                                              â”‚
â”‚                                                                          â”‚
â”‚  Processing Pipeline:                                                   â”‚
â”‚  1. Read from Kafka topic                                              â”‚
â”‚  2. Parse JSON messages                                                 â”‚
â”‚  3. Apply transformations (UDFs):                                      â”‚
â”‚     â”œâ”€ Text cleaning (remove URLs, special chars)                     â”‚
â”‚     â”œâ”€ Sentiment analysis (polarity: -1 to 1)                         â”‚
â”‚     â”œâ”€ Topic classification (8 categories)                             â”‚
â”‚     â”œâ”€ Entity extraction (countries, orgs)                             â”‚
â”‚     â””â”€ Feature engineering (word counts, engagement)                   â”‚
â”‚  4. Write to PostgreSQL (micro-batch)                                  â”‚
â”‚                                                                          â”‚
â”‚  Fault Tolerance:                                                       â”‚
â”‚  - Checkpointing for exactly-once processing                           â”‚
â”‚  - WAL (Write-Ahead Log) for recovery                                  â”‚
â”‚  - Automatic retry on failure                                          â”‚
â”‚                                                                          â”‚
â”‚  Monitoring: Spark Master UI (http://localhost:8081)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      POSTGRESQL DATA WAREHOUSE                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Database: reddit_db                                          â”‚      â”‚
â”‚  â”‚  Main Table: reddit_posts (20+ columns)                      â”‚      â”‚
â”‚  â”‚  Indexes: 10 indexes for query optimization                  â”‚      â”‚
â”‚  â”‚  Materialized Views: 3 pre-aggregated views                  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                          â”‚
â”‚  Schema:                                                                â”‚
â”‚  - Primary Key: post_id                                                 â”‚
â”‚  - Partitioning: By subreddit (optional: by date)                     â”‚
â”‚  - Indexes: subreddit, date, topic, sentiment, score                  â”‚
â”‚                                                                          â”‚
â”‚  Materialized Views:                                                    â”‚
â”‚  1. mv_daily_subreddit_stats                                           â”‚
â”‚  2. mv_hourly_sentiment                                                 â”‚
â”‚  3. mv_topic_performance                                                â”‚
â”‚                                                                          â”‚
â”‚  Access: PgAdmin (http://localhost:5050)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       ANALYTICS & VISUALIZATION                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚   Grafana   â”‚    â”‚   Tableau   â”‚    â”‚  Custom BI  â”‚               â”‚
â”‚  â”‚  Dashboards â”‚    â”‚   Reports   â”‚    â”‚   Tools     â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                          â”‚
â”‚  Grafana: http://localhost:3000                                        â”‚
â”‚  - Real-time metrics                                                    â”‚
â”‚  - Data quality dashboards                                              â”‚
â”‚  - Pipeline monitoring                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”„ Data Flow Sequence

```
1. INGESTION (Airflow Task: fetch_reddit_data)
   â””â”€> Fetch from Reddit API
   â””â”€> Save to temporary file
   â””â”€> Push filepath to XCom

2. KAFKA PRODUCTION (Airflow Task: produce_to_kafka)
   â””â”€> Read from temporary file
   â””â”€> Serialize to JSON
   â””â”€> Compress with gzip
   â””â”€> Send to Kafka topic "reddit-posts"
   â””â”€> Wait for broker acknowledgment

3. KAFKA STORAGE
   â””â”€> Store in topic partitions
   â””â”€> Replicate to backup brokers
   â””â”€> Retain for 24 hours
   â””â”€> Available for multiple consumers

4. SPARK CONSUMPTION (Long-running job)
   â””â”€> Read from Kafka (10s micro-batches)
   â””â”€> Deserialize JSON messages
   â””â”€> Checkpoint offset for fault tolerance

5. SPARK PROCESSING
   â””â”€> Text Cleaning
       â”œâ”€> Lowercase conversion
       â”œâ”€> URL removal
       â””â”€> Special character removal
   
   â””â”€> Sentiment Analysis (UDF)
       â”œâ”€> Count positive words
       â”œâ”€> Count negative words
       â””â”€> Calculate polarity score
   
   â””â”€> Topic Classification (UDF)
       â”œâ”€> Match keywords to categories
       â””â”€> Assign primary topic
   
   â””â”€> Entity Extraction (UDF)
       â”œâ”€> Search for country names
       â”œâ”€> Search for organizations
       â””â”€> Return entity list
   
   â””â”€> Feature Engineering
       â”œâ”€> Word counts
       â”œâ”€> Engagement categories
       â””â”€> Time-based features

6. POSTGRESQL WRITE (Spark foreachBatch)
   â””â”€> Format data for JDBC
   â””â”€> Batch insert (micro-batch)
   â””â”€> Update indexes
   â””â”€> Trigger auto-vacuum

7. ANALYTICS QUERIES (Airflow Task: run_analytics)
   â””â”€> Refresh materialized views
   â””â”€> Run aggregation queries
   â””â”€> Update statistics tables

8. VALIDATION (Airflow Task: validate_postgres_data)
   â””â”€> Count records
   â””â”€> Check data quality
   â””â”€> Calculate metrics

9. NOTIFICATION (Airflow Task: send_success_notification)
   â””â”€> Collect statistics
   â””â”€> Send to monitoring system
   â””â”€> Log completion
```

---

## âš™ï¸ Component Details

### 1. Airflow (Orchestration)

**Version**: 2.7.3  
**Executor**: LocalExecutor (for demo, use CeleryExecutor in production)  
**Backend**: PostgreSQL (metadata database)

**DAG Configuration**:
- **Name**: `reddit_streaming_pipeline`
- **Schedule**: `0 */4 * * *` (every 4 hours)
- **Catchup**: False
- **Max Active Runs**: 1
- **Retries**: 2
- **Retry Delay**: 5 minutes

**Tasks** (9 total):
1. `check_kafka_health` - Health check
2. `check_postgres_health` - Health check
3. `fetch_reddit_data` - Data ingestion
4. `produce_to_kafka` - Kafka production
5. `check_spark_streaming` - Verify Spark job
6. `wait_for_spark_processing` - Allow processing time
7. `validate_postgres_data` - Data validation
8. `run_analytics_queries` - SQL analytics
9. `send_success_notification` - Alerting

### 2. Kafka (Message Queue)

**Version**: 7.5.0 (Confluent Platform)  
**Zookeeper**: Required for cluster coordination

**Configuration**:
```
Brokers: 1 (scalable to N)
Topic: reddit-posts
Partitions: 3
Replication Factor: 1 (increase to 3 in production)
Retention: 24 hours
Compression: gzip
Max Message Size: 1MB
```

**Performance Tuning**:
- `batch.size=16384` (16KB)
- `linger.ms=10` (wait 10ms for batching)
- `compression.type=gzip`
- `acks=all` (full acknowledgment)

### 3. Spark (Stream Processing)

**Version**: 3.5.0  
**Mode**: Cluster (1 master + 2 workers)

**Cluster Configuration**:
```
Master: 1 node
  - Manages job scheduling
  - Runs driver program
  - UI: http://localhost:8081

Worker 1 & 2: 2 cores, 2GB RAM each
  - Execute tasks
  - Store partitioned data
  - Report to master
```

**Streaming Configuration**:
```python
.config("spark.sql.shuffle.partitions", "10")
.config("spark.streaming.kafka.maxRatePerPartition", "1000")
.config("spark.sql.adaptive.enabled", "true")
```

**UDF Functions**:
1. `clean_text_udf` - Text normalization
2. `sentiment_udf` - Sentiment scoring
3. `topic_udf` - Topic classification
4. `entities_udf` - Entity extraction

### 4. PostgreSQL (Data Warehouse)

**Version**: 14  
**Database**: reddit_db  
**User**: reddit_user

**Main Table**: `reddit_posts`
- 21 columns
- 10 indexes
- Auto-vacuum enabled
- Partitioning ready (optional)

**Materialized Views**:
1. `mv_daily_subreddit_stats` - Daily aggregations
2. `mv_hourly_sentiment` - Hourly sentiment trends
3. `mv_topic_performance` - Topic analytics

**Performance Settings**:
```sql
max_connections = 200
shared_buffers = 4GB
effective_cache_size = 12GB
work_mem = 64MB
```

---

## ðŸ“Š Monitoring Stack

### Airflow Monitoring
- **URL**: http://localhost:8080
- **Metrics**: Task success/failure, duration, schedules
- **Alerts**: Email on failure

### Kafka Monitoring
- **Kafka UI**: http://localhost:8090
- **Metrics**: Message throughput, lag, partition distribution
- **Alerts**: Consumer lag > threshold

### Spark Monitoring
- **Spark UI**: http://localhost:8081
- **Metrics**: Job stages, task execution, memory usage
- **Alerts**: Job failure, memory overflow

### PostgreSQL Monitoring
- **PgAdmin**: http://localhost:5050
- **Metrics**: Query performance, table sizes, index usage
- **Alerts**: Slow queries, disk space

### Grafana Dashboards
- **URL**: http://localhost:3000
- **Dashboards**:
  - Pipeline Overview
  - Data Quality Metrics
  - Performance Metrics
  - Error Rates

---

## ðŸ”’ Security Considerations

### Implemented:
- âœ… Network isolation (Docker network)
- âœ… PostgreSQL user permissions
- âœ… Kafka topic access control (ready)

### Production TODO:
- [ ] Enable Kafka SASL/SSL authentication
- [ ] PostgreSQL SSL connections
- [ ] Airflow RBAC (Role-Based Access Control)
- [ ] Secrets management (Vault/AWS Secrets Manager)
- [ ] Network policies/firewalls
- [ ] Audit logging
- [ ] Data encryption at rest

---

## ðŸ“ˆ Scalability Path

### Current (Demo):
- **Throughput**: ~1000 messages/second
- **Data Volume**: ~1GB/day
- **Latency**: ~10 seconds (micro-batch)

### Stage 1 (Small Production):
- Add 2 more Kafka brokers (3 total)
- Add 2 more Spark workers (4 total)
- Increase worker resources (4 cores, 4GB each)
- Expected: 5000 msg/sec, ~5GB/day

### Stage 2 (Medium Production):
- Kafka cluster: 5 brokers
- Spark cluster: 10 workers
- PostgreSQL: Read replicas
- Expected: 20K msg/sec, ~20GB/day

### Stage 3 (Large Production):
- Kafka cluster: 10+ brokers
- Spark on EMR/Dataproc (auto-scaling)
- PostgreSQL: Partitioned + read replicas
- Add: Data lake (S3/GCS) for long-term storage
- Expected: 100K+ msg/sec, 100GB+/day

---

## ðŸ’° Cost Estimation

### Local Development (Docker):
**Cost**: $0 (uses your machine)

### AWS Production (Medium):
```
Kafka (MSK): 3 kafka.m5.large = $600/month
Spark (EMR): 10 m5.xlarge spot = $500/month
PostgreSQL (RDS): db.r5.xlarge = $400/month
Airflow (MWAA): 1 environment = $300/month
Total: ~$1800/month
```

### Optimization Tips:
- Use spot instances (70% cheaper)
- Auto-scaling for variable workloads
- Reserved instances for baseline capacity
- S3 for cold storage (cheaper than RDS)

---

## ðŸŽ¯ Use Cases & Applications

### 1. Real-time News Monitoring
- Track breaking news across subreddits
- Alert on viral posts
- Sentiment tracking for crisis management

### 2. Brand Monitoring
- Monitor brand mentions
- Track sentiment shifts
- Competitive analysis

### 3. Trend Analysis
- Identify emerging topics
- Track topic evolution
- Predict viral content

### 4. Content Recommendation
- Personalized feed generation
- Topic-based filtering
- Engagement prediction

### 5. Research & Academia
- Social media discourse analysis
- Political sentiment tracking
- Misinformation spread patterns

---

## ðŸ”§ Maintenance & Operations

### Daily:
- Monitor Airflow DAG runs
- Check Kafka consumer lag
- Review error logs

### Weekly:
- Refresh materialized views
- Vacuum PostgreSQL tables
- Review performance metrics

### Monthly:
- Update dependencies
- Review and optimize queries
- Capacity planning
- Security patches

### Quarterly:
- Architecture review
- Cost optimization
- Disaster recovery testing
- Scale testing

---

## ðŸ“š Technology Stack Summary

| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| **Orchestration** | Apache Airflow | 2.7.3 | Workflow scheduling |
| **Message Queue** | Apache Kafka | 7.5.0 | Streaming ingestion |
| **Stream Processing** | Apache Spark | 3.5.0 | Real-time ETL |
| **Data Warehouse** | PostgreSQL | 14 | Analytical storage |
| **Containerization** | Docker | 20.10+ | Deployment |
| **Monitoring** | Grafana | Latest | Dashboards |
| **Programming** | Python | 3.9+ | Pipeline code |

---

**ðŸŽ‰ Complete production-ready streaming architecture!**

This architecture demonstrates modern Big Data engineering best practices and can scale from local development to enterprise production.
