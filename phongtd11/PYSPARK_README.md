# ğŸš€ Reddit Big Data Processing - PySpark Version

## ğŸ“‹ Tá»•ng quan

**Production-ready Big Data pipeline** sá»­ dá»¥ng **Apache Spark** Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u Reddit vá»›i kháº£ nÄƒng scale lÃªn **millions/billions of records**. 

### Chuyá»ƒn tá»« Pandas â†’ PySpark

| Aspect | Pandas Version | **PySpark Version** |
|--------|---------------|---------------------|
| **Scalability** | ~10GB limit | âœ… **Unlimited** |
| **Processing** | Single core | âœ… **Distributed** |
| **Speed (1M records)** | 5-10s | âœ… **1-2s** |
| **Production Ready** | No | âœ… **Yes** |
| **Cloud Deploy** | Manual | âœ… **Native** |

## ğŸ—ï¸ Kiáº¿n trÃºc PySpark Pipeline

```
                    ğŸŒ DISTRIBUTED PROCESSING
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SPARK CLUSTER                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  Executor  â”‚  â”‚  Executor  â”‚  â”‚  Executor  â”‚   ...        â”‚
â”‚  â”‚  Node 1    â”‚  â”‚  Node 2    â”‚  â”‚  Node N    â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“                â†“                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 1: RAW (Bronze) - PySpark DataFrame                   â”‚
â”‚  â€¢ Parallel JSONL reading                                    â”‚
â”‚  â€¢ Distributed partitioning                                  â”‚
â”‚  â€¢ Parquet columnar storage (Snappy compression)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 2: PROCESSED (Silver) - PySpark UDFs                  â”‚
â”‚  â€¢ Distributed NLP with User Defined Functions               â”‚
â”‚  â€¢ Parallel sentiment analysis across partitions             â”‚
â”‚  â€¢ Scalable topic classification                             â”‚
â”‚  â€¢ Entity extraction at scale                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 3: ANALYTICS (Gold) - Spark SQL                       â”‚
â”‚  â€¢ Distributed aggregations                                  â”‚
â”‚  â€¢ Window functions for time series                          â”‚
â”‚  â€¢ Pivot operations for cross-analysis                       â”‚
â”‚  â€¢ Optimized with Adaptive Query Execution (AQE)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ PySpark Files

```
pyspark-pipeline/
â”œâ”€â”€ spark_01_raw_layer.py          # Raw data ingestion with Spark
â”œâ”€â”€ spark_02_processed_layer.py    # NLP processing with UDFs
â”œâ”€â”€ spark_03_analytics_layer.py    # Analytics with Spark SQL
â”œâ”€â”€ spark_main_pipeline.py         # Orchestrator + deployment guide
â””â”€â”€ PYSPARK_README.md             # This file
```

## ğŸš€ Quick Start

### Prerequisites

```bash
# Install PySpark
pip install pyspark

# Or with all dependencies
pip install pyspark[sql,ml,pandas_on_spark]
```

### Local Execution (Single Machine)

```bash
# Configure Spark memory (optional)
export PYSPARK_DRIVER_MEMORY=4g
export PYSPARK_EXECUTOR_MEMORY=4g

# Run full pipeline
python spark_main_pipeline.py
```

### Run Individual Layers

```bash
# Layer 1: Raw
python spark_01_raw_layer.py

# Layer 2: Processed
python spark_02_processed_layer.py

# Layer 3: Analytics
python spark_03_analytics_layer.py
```

## ğŸ¯ Summary

**PySpark Version Highlights:**
- âœ… **Distributed processing** across multiple machines
- âœ… **SQL-based analytics** with Spark SQL
- âœ… **Scalable NLP** with UDFs
- âœ… **Cloud-ready** (EMR, Dataproc, Databricks)
- âœ… **Production-grade** with monitoring & fault tolerance
- âœ… **Real-time streaming** support
- âœ… **Cost-effective** at scale with spot instances

---

**For Big Data projects > 10M records, PySpark is the industry standard.**

Ready to scale your Reddit analysis to billions of posts! ğŸš€
