"""
MAIN PYSPARK PIPELINE - Reddit Big Data Processing
Orchestrate toÃ n bá»™ Spark pipeline: Raw â†’ Processed â†’ Analytics
Production-ready, scalable to millions of records
"""

import sys
import time
from datetime import datetime
from pathlib import Path


class SparkPipeline:
    def __init__(self, input_file):
        self.input_file = input_file
        self.start_time = datetime.now()
        
    def run(self):
        """Execute full Spark pipeline"""
        print("="*80)
        print("ğŸš€ REDDIT BIG DATA PROCESSING - PYSPARK PIPELINE")
        print("="*80)
        print(f"ğŸ“… Started at: {self.start_time}")
        print(f"ğŸ“ Input file: {self.input_file}")
        print(f"âš¡ Framework: Apache Spark (Distributed Processing)")
        print("="*80)
        
        results = {}
        
        try:
            # ================================================================
            # LAYER 1: RAW DATA INGESTION
            # ================================================================
            print("\n\n")
            print("â–ˆ" * 80)
            print("â–ˆ  LAYER 1: RAW DATA INGESTION (BRONZE) - PYSPARK                              â–ˆ")
            print("â–ˆ" * 80)
            
            from bai_tap_lon.phongtd11.spark_scripts.spark_01_raw_layer import RawLayerProcessorSpark
            
            raw_processor = RawLayerProcessorSpark(
                input_file=self.input_file,
                output_dir="data_spark/raw"
            )
            
            df_raw, manifest_raw = raw_processor.process()
            raw_processor.stop()
            
            print(f"\nâœ… RAW LAYER COMPLETE (PySpark)")
            print(f"   ğŸ“Š Records: {manifest_raw['total_records']}")
            print(f"   ğŸ“¦ Output: data_spark/raw/")
            
            results['raw_count'] = manifest_raw['total_records']
            results['raw_manifest'] = manifest_raw
            
            # ================================================================
            # LAYER 2: PROCESSED DATA
            # ================================================================
            print("\n\n")
            print("â–ˆ" * 80)
            print("â–ˆ  LAYER 2: PROCESSED DATA (SILVER) - PYSPARK                                  â–ˆ")
            print("â–ˆ" * 80)
            
            from bai_tap_lon.phongtd11.spark_scripts.spark_02_processed_layer import ProcessedLayerProcessorSpark
            
            processed_processor = ProcessedLayerProcessorSpark(
                raw_data_path="data_spark/raw",
                output_dir="data_spark/processed"
            )
            
            df_processed = processed_processor.process()
            processed_count = df_processed.count()
            processed_processor.stop()
            
            print(f"\nâœ… PROCESSED LAYER COMPLETE (PySpark)")
            print(f"   ğŸ“Š Records: {processed_count}")
            print(f"   ğŸ“¦ Output: data_spark/processed/")
            
            results['processed_count'] = processed_count
            
            # ================================================================
            # LAYER 3: ANALYTICS
            # ================================================================
            print("\n\n")
            print("â–ˆ" * 80)
            print("â–ˆ  LAYER 3: ANALYTICS (GOLD) - PYSPARK SQL                                     â–ˆ")
            print("â–ˆ" * 80)
            
            from bai_tap_lon.phongtd11.spark_scripts.spark_03_analytics_layer import AnalyticsLayerProcessorSpark
            
            analytics_processor = AnalyticsLayerProcessorSpark(
                processed_data_path="data_spark/processed",
                output_dir="data_spark/analytics"
            )
            
            tables, insights = analytics_processor.process()
            analytics_processor.stop()
            
            print(f"\nâœ… ANALYTICS LAYER COMPLETE (PySpark SQL)")
            print(f"   ğŸ“Š Tables: {len(tables)}")
            print(f"   ğŸ’¡ Insights: {len(insights['key_insights'])}")
            print(f"   ğŸ“¦ Output: data_spark/analytics/")
            
            results['tables_count'] = len(tables)
            results['insights'] = insights
            
            # ================================================================
            # FINAL SUMMARY
            # ================================================================
            end_time = datetime.now()
            duration = (end_time - self.start_time).total_seconds()
            
            print("\n\n")
            print("="*80)
            print("ğŸ‰ PYSPARK PIPELINE EXECUTION COMPLETE!")
            print("="*80)
            print(f"â±ï¸  Total Duration: {duration:.2f} seconds")
            print(f"ğŸ“Š Total Records Processed: {results['processed_count']:,}")
            print(f"âš¡ Processing Speed: {results['processed_count']/duration:.0f} records/second")
            print(f"\nğŸ—‚ï¸  Data Layers Created:")
            print(f"   âœ“ Raw Layer (Bronze) - Partitioned Parquet")
            print(f"   âœ“ Processed Layer (Silver) - NLP Enriched")
            print(f"   âœ“ Analytics Layer (Gold) - 7 Analytical Tables")
            
            print(f"\nğŸ“ PySpark Output Structure:")
            print(f"   data_spark/")
            print(f"   â”œâ”€â”€ raw/                         # Partitioned by subreddit/year/month/day")
            print(f"   â”œâ”€â”€ processed/                   # Partitioned by subreddit")
            print(f"   â””â”€â”€ analytics/                   # 7 analytical tables")
            print(f"       â”œâ”€â”€ daily_subreddit_stats/")
            print(f"       â”œâ”€â”€ trending_topics/")
            print(f"       â”œâ”€â”€ user_engagement/")
            print(f"       â”œâ”€â”€ cross_subreddit_comparison/")
            print(f"       â”œâ”€â”€ hourly_patterns/")
            print(f"       â”œâ”€â”€ entity_network/")
            print(f"       â”œâ”€â”€ sentiment_timeseries/")
            print(f"       â””â”€â”€ insights.json")
            
            print(f"\nğŸš€ Scalability:")
            print(f"   â€¢ Current: {results['processed_count']} posts in {duration:.1f}s")
            print(f"   â€¢ Spark can scale to: Millions of posts")
            print(f"   â€¢ Distributed: Across multiple nodes/cores")
            print(f"   â€¢ Production: Deploy on EMR, Databricks, Dataproc")
            
            print(f"\nğŸ’¡ Key Insights Generated:")
            for i, insight in enumerate(insights['key_insights'], 1):
                print(f"   {i}. {insight['title']}")
            
            print("="*80)
            
            results['duration'] = duration
            results['throughput'] = results['processed_count'] / duration
            
            return results
            
        except Exception as e:
            print(f"\nâŒ ERROR: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

# ============================================================================
# EXECUTION
# ============================================================================
if __name__ == "__main__":

    print("\n" + "="*80)
    print("Starting PySpark Pipeline...")
    print("="*80 + "\n")
    
    # Run pipeline
    pipeline = SparkPipeline(
        input_file="/mnt/user-data/uploads/reddit_posts.jsonl"
    )
    
    results = pipeline.run()
    
    if results:
        print("\n\nğŸ’¾ RESULTS SUMMARY:")
        print(f"   â€¢ Raw records: {results['raw_count']:,}")
        print(f"   â€¢ Processed records: {results['processed_count']:,}")
        print(f"   â€¢ Analytical tables: {results['tables_count']}")
        print(f"   â€¢ Processing time: {results['duration']:.2f}s")
        print(f"   â€¢ Throughput: {results['throughput']:.0f} records/sec")
        print(f"\nâœ… All outputs saved to: data_spark/")
