"""
MAIN PYSPARK PIPELINE - Reddit Big Data Processing
Orchestrate to√†n b·ªô Spark pipeline: Raw ‚Üí Processed ‚Üí Analytics
Production-ready, scalable to millions of records
"""

import sys
import time
from datetime import datetime
from pathlib import Path


class SparkPipeline:
    def __init__(self, input_file):
        self.input_file = input_file
        self.start_time = datetime.now()
        
    def run(self):
        """Execute full Spark pipeline"""
        print("="*80)
        print("üöÄ REDDIT BIG DATA PROCESSING - PYSPARK PIPELINE")
        print("="*80)
        print(f"üìÖ Started at: {self.start_time}")
        print(f"üìÅ Input file: {self.input_file}")
        print(f"‚ö° Framework: Apache Spark (Distributed Processing)")
        print("="*80)
        
        results = {}
        
        try:
            # ================================================================
            # LAYER 1: RAW DATA INGESTION
            # ================================================================
            print("\n\n")
            print("‚ñà" * 80)
            print("‚ñà  LAYER 1: RAW DATA INGESTION (BRONZE) - PYSPARK                              ‚ñà")
            print("‚ñà" * 80)
            
            from bai_tap_lon.phongtd11.spark_scripts.spark_01_raw_layer import RawLayerProcessorSpark
            
            raw_processor = RawLayerProcessorSpark(
                input_file=self.input_file,
                output_dir="data_spark/raw"
            )
            
            df_raw, manifest_raw = raw_processor.process()
            raw_processor.stop()
            
            print(f"\n‚úÖ RAW LAYER COMPLETE (PySpark)")
            print(f"   üìä Records: {manifest_raw['total_records']}")
            print(f"   üì¶ Output: data_spark/raw/")
            
            results['raw_count'] = manifest_raw['total_records']
            results['raw_manifest'] = manifest_raw
            
            # ================================================================
            # LAYER 2: PROCESSED DATA
            # ================================================================
            print("\n\n")
            print("‚ñà" * 80)
            print("‚ñà  LAYER 2: PROCESSED DATA (SILVER) - PYSPARK                                  ‚ñà")
            print("‚ñà" * 80)
            
            from bai_tap_lon.phongtd11.spark_scripts.spark_02_processed_layer import ProcessedLayerProcessorSpark
            
            processed_processor = ProcessedLayerProcessorSpark(
                raw_data_path="data_spark/raw",
                output_dir="data_spark/processed"
            )
            
            df_processed = processed_processor.process()
            processed_count = df_processed.count()
            processed_processor.stop()
            
            print(f"\n‚úÖ PROCESSED LAYER COMPLETE (PySpark)")
            print(f"   üìä Records: {processed_count}")
            print(f"   üì¶ Output: data_spark/processed/")
            
            results['processed_count'] = processed_count
            
            # ================================================================
            # LAYER 3: ANALYTICS
            # ================================================================
            print("\n\n")
            print("‚ñà" * 80)
            print("‚ñà  LAYER 3: ANALYTICS (GOLD) - PYSPARK SQL                                     ‚ñà")
            print("‚ñà" * 80)
            
            from bai_tap_lon.phongtd11.spark_scripts.spark_03_analytics_layer import AnalyticsLayerProcessorSpark
            
            analytics_processor = AnalyticsLayerProcessorSpark(
                processed_data_path="data_spark/processed",
                output_dir="data_spark/analytics"
            )
            
            tables, insights = analytics_processor.process()
            analytics_processor.stop()
            
            print(f"\n‚úÖ ANALYTICS LAYER COMPLETE (PySpark SQL)")
            print(f"   üìä Tables: {len(tables)}")
            print(f"   üí° Insights: {len(insights['key_insights'])}")
            print(f"   üì¶ Output: data_spark/analytics/")
            
            results['tables_count'] = len(tables)
            results['insights'] = insights
            
            # ================================================================
            # FINAL SUMMARY
            # ================================================================
            end_time = datetime.now()
            duration = (end_time - self.start_time).total_seconds()
            
            print("\n\n")
            print("="*80)
            print("üéâ PYSPARK PIPELINE EXECUTION COMPLETE!")
            print("="*80)
            print(f"‚è±Ô∏è  Total Duration: {duration:.2f} seconds")
            print(f"üìä Total Records Processed: {results['processed_count']:,}")
            print(f"‚ö° Processing Speed: {results['processed_count']/duration:.0f} records/second")
            print(f"\nüóÇÔ∏è  Data Layers Created:")
            print(f"   ‚úì Raw Layer (Bronze) - Partitioned Parquet")
            print(f"   ‚úì Processed Layer (Silver) - NLP Enriched")
            print(f"   ‚úì Analytics Layer (Gold) - 7 Analytical Tables")
            
            print(f"\nüí° Key Insights Generated:")
            for i, insight in enumerate(insights['key_insights'], 1):
                print(f"   {i}. {insight['title']}")
            
            print("="*80)
            
            results['duration'] = duration
            results['throughput'] = results['processed_count'] / duration
            
            return results
            
        except Exception as e:
            print(f"\n‚ùå ERROR: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

# ============================================================================
# EXECUTION
# ============================================================================
if __name__ == "__main__":

    print("\n" + "="*80)
    print("Starting PySpark Pipeline...")
    print("="*80 + "\n")
    
    # Run pipeline
    pipeline = SparkPipeline(
        input_file="/mnt/user-data/uploads/reddit_posts.jsonl"
    )
    
    results = pipeline.run()
    
    if results:
        print("\n\nüíæ RESULTS SUMMARY:")
        print(f"   ‚Ä¢ Raw records: {results['raw_count']:,}")
        print(f"   ‚Ä¢ Processed records: {results['processed_count']:,}")
        print(f"   ‚Ä¢ Analytical tables: {results['tables_count']}")
        print(f"   ‚Ä¢ Processing time: {results['duration']:.2f}s")
        print(f"   ‚Ä¢ Throughput: {results['throughput']:.0f} records/sec")
        print(f"\n‚úÖ All outputs saved to: data_spark/")
